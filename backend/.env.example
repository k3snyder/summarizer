# =============================================================================
# Document Summarizer Pipeline - Backend Configuration
# =============================================================================
# Copy this file to .env and customize the values for your environment.
# All values shown are defaults.

# -----------------------------------------------------------------------------
# Server Configuration
# -----------------------------------------------------------------------------
HOST=0.0.0.0
PORT=8000
CORS_ORIGINS=http://localhost:3000,http://localhost:3001,http://localhost:3002,http://localhost:3003

# -----------------------------------------------------------------------------
# Database Configuration
# -----------------------------------------------------------------------------
DATABASE_URL=sqlite+aiosqlite:///./jobs.db

# -----------------------------------------------------------------------------
# Job Management
# -----------------------------------------------------------------------------
JOB_TEMP_DIR=/tmp/summarizer-jobs
JOB_RETENTION_HOURS=24
CLEANUP_INTERVAL_MINUTES=60
CLEANUP_MAX_AGE_HOURS=24

# -----------------------------------------------------------------------------
# Ollama Configuration (Local LLM)
# -----------------------------------------------------------------------------
# Primary Ollama instance URL
OLLAMA_BASE_URL=http://localhost:11434/v1

# Optional secondary Ollama instances for load balancing
OLLAMA_BASE_URL_1=http://localhost:11434/v1
OLLAMA_BASE_URL_2=http://localhost:11434/v1

# -----------------------------------------------------------------------------
# Vision Processing Configuration
# -----------------------------------------------------------------------------
# Provider: "ollama", "openai", or "gemini"
VISION_PROVIDER=ollama

# Model for vision processing (Ollama)
VISION_MODEL=ministral-3:latest

# -----------------------------------------------------------------------------
# OpenAI Configuration (Optional - for OpenAI vision provider)
# -----------------------------------------------------------------------------
# Required only if using OpenAI for vision processing
OPENAI_API_KEY=
OPENAI_VISION_MODEL=gpt-4.1-mini

# -----------------------------------------------------------------------------
# Google Gemini Configuration (Optional - for Gemini vision provider)
# -----------------------------------------------------------------------------
# Required only if using Gemini for vision processing
GEMINI_API_KEY=
GEMINI_VISION_MODEL=gemini-2.5-flash

# -----------------------------------------------------------------------------
# API Retry/Timeout Configuration
# -----------------------------------------------------------------------------
API_MAX_RETRIES=3
API_RETRY_DELAY=5.0
API_REQUEST_TIMEOUT=120.0

# -----------------------------------------------------------------------------
# Summarization Configuration
# -----------------------------------------------------------------------------
# Model tiers for fallback rotation (attempts 1-10, 11-20, 21-30)
SUMMARIZER_MODEL_TIER_1=ministral-3:latest
SUMMARIZER_MODEL_TIER_2=ministral-3:latest
SUMMARIZER_MODEL_TIER_3=ministral-3:latest

# Quality thresholds (percentage)
SUMMARIZER_QUALITY_THRESHOLD_HIGH=90
SUMMARIZER_QUALITY_THRESHOLD_LOW=85

# Maximum retry attempts
SUMMARIZER_MAX_ATTEMPTS=30

# -----------------------------------------------------------------------------
# Logging Configuration
# -----------------------------------------------------------------------------
# Global log level: DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO

# Per-stage log level overrides (optional - defaults to LOG_LEVEL)
# LOG_LEVEL_EXTRACTION=DEBUG
# LOG_LEVEL_VISION=INFO
# LOG_LEVEL_SUMMARIZATION=INFO

# Log file settings
LOG_DIR=./logs
LOG_MAX_BYTES=10485760
LOG_BACKUP_COUNT=5

# Enable debug logging for CLI operations
LOG_LEVEL_CLI=DEBUG

# Configure reasoning (to test medium model issue)
CODEX_CLI_REASONING_EFFORT=medium

# Increase stream timeout if experiencing disconnects
CODEX_CLI_STREAM_IDLE_TIMEOUT_MS=600000
